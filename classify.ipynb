{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrodd0FJFeqLt+LspCf+aY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girishcx/contentanalysis/blob/master/classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "l4zhmk40zHV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import os, stat\n",
        "import json\n",
        "from   typing import Tuple\n",
        "import subprocess\n",
        "import pathlib\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from torch import cuda"
      ],
      "metadata": {
        "id": "Dnj7QGi1zDy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "\n",
        "if use_cuda:\n",
        "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
        "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
      ],
      "metadata": {
        "id": "nkBm31Ply4iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "\n",
        "def update_cat(x):\n",
        "    return my_dict[x]\n",
        "\n",
        "encode_dict = {}\n",
        "def encode_cat(x):\n",
        "    if x not in encode_dict.keys():\n",
        "        encode_dict[x]=len(encode_dict)\n",
        "    return encode_dict[x]"
      ],
      "metadata": {
        "id": "NCI3aQtzy4lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = None\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.data.TITLE[index])\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "v1Nx4qU6y4oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 4)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "metadata": {
        "id": "hRmzny_L0l6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "\n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples\n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "    return"
      ],
      "metadata": {
        "id": "zseE2DgF03qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    tr_loss = 0;\n",
        "    n_correct = 0;\n",
        "    nb_tr_steps = 0;\n",
        "    nb_tr_examples = 0;\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "\n",
        "            if _%5000==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples\n",
        "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return epoch_accu"
      ],
      "metadata": {
        "id": "eZuyQt8u04QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training():\n",
        "    print(\"started training\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        train(epoch)\n",
        "\n",
        "    print('This is the validation section to print the accuracy and see how it performs')\n",
        "    print('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n",
        "    acc = valid(model, testing_loader)\n",
        "    print(\"Accuracy on test data = %0.2f%%\" % acc)\n",
        "\n",
        "    # Saving the files for re-use\n",
        "    output_model_file = './models/pytorch_distilbert_news.bin'\n",
        "    output_vocab_file = './models/vocab_distilbert_news.bin'\n",
        "    model_to_save = model\n",
        "    torch.save(model_to_save, output_model_file)\n",
        "    tokenizer.save_vocabulary(output_vocab_file)\n",
        "    print('model files saved')"
      ],
      "metadata": {
        "id": "-rJLr7xW0mD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-pH2OJElabcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NeQvtMZSagoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7OjjSjmnag1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Setting up the device for GPU usage\n",
        "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "    print(device)\n",
        "\n",
        "    # Import the csv into pandas dataframe and add the headers\n",
        "    #df = pd.read_csv('./data/newsdata10k.csv', sep='\\t', names=['ID','TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "    df = pd.read_csv('./bbc-text.csv')\n",
        "    df['ENCODE_CAT'] = df['category'].astype('category').cat.codes\n",
        "    df['TITLE'] = df['text']\n",
        "    df = df[['TITLE', 'ENCODE_CAT']]\n",
        "    # df.head()\n",
        "    # Removing unwanted columns and only leaving title of news and the category which will be the target\n",
        "    # df = df[['TITLE','CATEGORY']]\n",
        "    # # Converting the codes to appropriate categories using a dictionary\n",
        "    # my_dict = {\n",
        "    #     'e':'Entertainment',\n",
        "    #     'b':'Business',\n",
        "    #     't':'Science',\n",
        "    #     'm':'Health'\n",
        "    # }\n",
        "    # df['CATEGORY'] = df['CATEGORY'].apply(lambda x: update_cat(x))\n",
        "    # df['ENCODE_CAT'] = df['CATEGORY'].apply(lambda x: encode_cat(x))\n",
        "    print(df)\n",
        "\n",
        "    train_size = 0.8\n",
        "    train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "    test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    print(\"FULL Dataset: {}\".format(df.shape))\n",
        "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "    training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "    testing_set = Triage(test_dataset, tokenizer, MAX_LEN)\n",
        "    train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "    \n",
        "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    testing_loader = DataLoader(testing_set, **test_params)\n",
        "    model = DistillBERTClass()\n",
        "    model.to(device)\n",
        "\n",
        "    # Creating the loss function and optimizer\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "5W166QMKy4q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"FULL Dataset: {}\".format(df.shape))\n",
        "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    print(\"TEST Dataset: {}\".format(test_dataset.shape))"
      ],
      "metadata": {
        "id": "ylT6gYSKPCJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training()"
      ],
      "metadata": {
        "id": "Qp3kqOEOy4wO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}